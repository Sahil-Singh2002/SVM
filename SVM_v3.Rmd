---
title: "SVM and Classification Methods"
author: "Sahil"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Required Libraries
```{r}
# Load necessary libraries
library(ggplot2)
library(MASS)     # For LDA/QDA
library(e1071)    # For SVM and Naive Bayes
library(nnet)     # For multinomial logistic regression
library(reshape2) # For reshaping data
library(caret)    # For cross-validation
#library(knitr)
#library(kableExtra)


# Load the dataset
load("C:/Users/pmyss22/OneDrive - The University of Nottingham/Desktop/SML/Group Poject/zipCodeAllDigits.RData")

# Fixed color palette for consistent coloring
fixed_colors <- c(
  "0" = "#1f77b4", "1" = "#ff7f0e", "2" = "#2ca02c",
  "3" = "#d62728", "4" = "#9467bd", "5" = "#8c564b",
  "6" = "#e377c2", "7" = "#7f7f7f", "8" = "#bcbd22", "9" = "#17becf"
)

# Dimensionality reduction using PCA if needed
if (ncol(train.X) > 2) {
  pca <- prcomp(train.X, center = TRUE, scale. = TRUE)
  train.X.reduced <- pca$x[, 1:2]
  test.X.reduced <- predict(pca, test.X)[, 1:2]
} else {
  train.X.reduced <- train.X
  test.X.reduced <- test.X
}

# Data frames for training and testing
train.data <- data.frame(
  Feature1 = train.X.reduced[, 1],
  Feature2 = train.X.reduced[, 2],
  Label = as.factor(train.y)
)

test.data <- data.frame(
  Feature1 = test.X.reduced[, 1],
  Feature2 = test.X.reduced[, 2],
  Label = as.factor(test.y)
)

# Grid for decision boundary plots
create_prediction_grid <- function(train_data, test_data, step = 200) {
  x_min <- min(c(train_data$Feature1, test_data$Feature1)) - 1
  x_max <- max(c(train_data$Feature1, test_data$Feature1)) + 1
  y_min <- min(c(train_data$Feature2, test_data$Feature2)) - 1
  y_max <- max(c(train_data$Feature2, test_data$Feature2)) + 1
  
  expand.grid(
    Feature1 = seq(x_min, x_max, length.out = step),
    Feature2 = seq(y_min, y_max, length.out = step)
  )
}

# plot decision boundaries
plot_decision_boundaries <- function(train_data, test_data, model, grid, train_error_cv, test_error, title_prefix) {
  # Predict labels on the grid
  if ("class" %in% names(predict(model, grid))) {
    # models like LDA, QDA
    grid$Label <- predict(model, grid)$class
  } else {
    # models like Naive Bayes, SVM
    grid$Label <- predict(model, grid)
  }
  
  # numeric version of the labels for contour plotting
  grid$Label_Num <- as.numeric(as.factor(grid$Label))
  
  # generate the plot
  ggplot() +
    # coloured regions based on predictions
    geom_tile(data = grid, aes(x = Feature1, y = Feature2, fill = Label), alpha = 0.3) +
    # add decision boundaries with black lines
    geom_contour(data = grid, aes(x = Feature1, y = Feature2, z = Label_Num), color = "black", size = 1) +
    # training points
    geom_point(data = train_data, aes(x = Feature1, y = Feature2, color = Label), alpha = 0.7) +
    # test points with a different shape
    geom_point(data = test_data, aes(x = Feature1, y = Feature2, color = Label), shape = 17, alpha = 0.7) +
    # fixed color
    scale_fill_manual(values = fixed_colors, name = "Regions") +
    scale_color_manual(values = fixed_colors, name = "Labels") +
    # legend
    theme_minimal() +
    theme(
      legend.position = "bottom",
      legend.text = element_text(size = 12),          # Increase legend text size
      legend.title = element_text(size = 14, face = "bold"),  # Increase legend title size
      plot.title = element_text(size = 18, face = "bold", hjust = 0.5)  # Increase title size and center it
    ) +
    # titles and axis labels
    labs(
      title = sprintf("%s\nCV Training Error: %.2f, Test Error: %.2f", title_prefix, train_error_cv, test_error),
      x = "Feature 1",
      y = "Feature 2",
      color = "Labels"
    )
}



# cross-validation fof k = 5
perform_cross_validation <- function(model_func, data, folds = 5) {
  ctrl <- trainControl(method = "cv", number = folds)
  cv_model <- train(Label ~ ., data = data, method = model_func, trControl = ctrl)
  return(cv_model$results$Accuracy)
}

# Initialize an empty data frame for MCE summary
mce_summary <- data.frame(Method = character(), Train_Error = numeric(), Test_Error = numeric(), CV_Accuracy = numeric())

grid <- create_prediction_grid(train.data, test.data)

# LDA
lda_model <- lda(Label ~ Feature1 + Feature2, data = train.data)
train_error <- mean(predict(lda_model, train.data)$class != train.data$Label)
test_error <- mean(predict(lda_model, test.data)$class != test.data$Label)
cv_accuracy <- perform_cross_validation("lda", train.data)
train_error_cv <- 1 - cv_accuracy  # Convert accuracy to error for the title

lda_plot <- plot_decision_boundaries(
  train_data = train.data,
  test_data = test.data,
  model = lda_model,
  grid = grid,
  train_error_cv = train_error_cv,
  test_error = test_error,
  title_prefix = "LDA Decision Boundary"
)
ggsave("lda_decision_boundary.png", plot = lda_plot, width = 10, height = 8)

# QDA
qda_model <- qda(Label ~ Feature1 + Feature2, data = train.data)
train_error <- mean(predict(qda_model, train.data)$class != train.data$Label)
test_error <- mean(predict(qda_model, test.data)$class != test.data$Label)
cv_accuracy <- perform_cross_validation("qda", train.data)
train_error_cv <- 1 - cv_accuracy  # Convert accuracy to error for the title

qda_plot <- plot_decision_boundaries(
  train_data = train.data,
  test_data = test.data,
  model = qda_model,
  grid = grid,
  train_error_cv = train_error_cv,
  test_error = test_error,
  title_prefix = "QDA Decision Boundary"
)
ggsave("qda_decision_boundary.png", plot = qda_plot, width = 10, height = 8)

# Naive Bayes
nb_model <- naiveBayes(Label ~ Feature1 + Feature2, data = train.data)
train_error <- mean(predict(nb_model, train.data) != train.data$Label)
test_error <- mean(predict(nb_model, test.data) != test.data$Label)
cv_accuracy <- perform_cross_validation("nb", train.data)
train_error_cv <- 1 - cv_accuracy  # Convert accuracy to error for the title

nb_plot <- plot_decision_boundaries(
  train_data = train.data,
  test_data = test.data,
  model = nb_model,
  grid = grid,
  train_error_cv = train_error_cv,
  test_error = test_error,
  title_prefix = "Naive Bayes Decision Boundary"
)
ggsave("naive_bayes_decision_boundary.png", plot = nb_plot, width = 10, height = 8)

# SVM with Radial Kernel
tune_radial <- tune(
  svm,
  Label ~ Feature1 + Feature2,
  data = train.data,
  kernel = "radial",
  ranges = list(cost = 2^(2:5), gamma = 2^(-4:-1))
)
svm_radial <- tune_radial$best.model
train_error <- mean(predict(svm_radial, train.data) != train.data$Label)
test_error <- mean(predict(svm_radial, test.data) != test.data$Label)
cv_accuracy <- perform_cross_validation("svmRadial", train.data)
train_error_cv <- 1 - cv_accuracy  # Convert accuracy to error for the title

svm_radial_plot <- plot_decision_boundaries(
  train_data = train.data,
  test_data = test.data,
  model = svm_radial,
  grid = grid,
  train_error_cv = train_error_cv,
  test_error = test_error,
  title_prefix = "SVM Radial Decision Boundary"
)
ggsave("svm_radial_decision_boundary.png", plot = svm_radial_plot, width = 10, height = 8)

# SVM with Polynomial Kernel (Degree 3)
tune_poly <- tune(
  svm,
  Label ~ Feature1 + Feature2,
  data = train.data,
  kernel = "polynomial",
  ranges = list(cost = 2^(2:5), gamma = 2^(-4:-1), coef0 = c(0, 1))
)
svm_poly <- tune_poly$best.model
svm_poly$degree <- 3  # Explicitly set the degree to 3 for the polynomial kernel

train_error <- mean(predict(svm_poly, train.data) != train.data$Label)
test_error <- mean(predict(svm_poly, test.data) != test.data$Label)
cv_accuracy <- perform_cross_validation("svmPoly", train.data)
train_error_cv <- 1 - cv_accuracy  # Convert accuracy to error for the title

svm_poly_plot <- plot_decision_boundaries(
  train_data = train.data,
  test_data = test.data,
  model = svm_poly,
  grid = grid,
  train_error_cv = train_error_cv,
  test_error = test_error,
  title_prefix = "SVM Polynomial (Degree 3) Decision Boundary"
)
ggsave("svm_polynomial_degree_3_decision_boundary.png", plot = svm_poly_plot, width = 10, height = 8)

# Multinomial Logistic Regression
log_model <- multinom(Label ~ Feature1 + Feature2, data = train.data)
train_error <- mean(predict(log_model, train.data) != train.data$Label)
test_error <- mean(predict(log_model, test.data) != test.data$Label)
cv_accuracy <- perform_cross_validation("multinom", train.data)
train_error_cv <- 1 - cv_accuracy  # Convert accuracy to error for the title

log_plot <- plot_decision_boundaries(
  train_data = train.data,
  test_data = test.data,
  model = log_model,
  grid = grid,
  train_error_cv = train_error_cv,
  test_error = test_error,
  title_prefix = "Multinomial Logistic Regression Decision Boundary"
)
ggsave("logistic_regression_decision_boundary.png", plot = log_plot, width = 10, height = 8)

```

```{r}
# Load necessary libraries

library(progress)   # For the progress bar

# Load the dataset
load("zipCodeAllDigits.RData")

# Standardize training data
XC <- scale(train.X, scale = FALSE)  # Center the data
cov.XC <- cov(XC)  # Covariance matrix
foo <- eigen(cov.XC)  # Eigen decomposition

# Create a grid of dimensions to evaluate
p.grid <- 1:195  # Number of dimensions to test

# Initialize results storage
results <- data.frame(
  Dimensions = integer(),
  MCE_test = numeric(),
  Method = character()
)

# Function to calculate MCE for a given method
calculate_mce <- function(method, train.data, test.data) {
  if (method == "LDA") {
    # Remove constant predictors within groups
    non_constant_cols <- sapply(train.data[, -ncol(train.data)], function(x) {
      length(unique(x)) > 1
    })

    if (all(!non_constant_cols)) {
      return(NA)  # If no valid predictors remain, return NA
    }

    # Subset the data to only non-constant columns
    train.data <- train.data[, c(non_constant_cols, TRUE)]
    test.data <- test.data[, c(non_constant_cols, TRUE)]

    # Fit LDA model
    model <- lda(Label ~ ., data = train.data)
    pred <- predict(model, test.data)$class
  } else if (method == "QDA") {
    # Wrap QDA in a tryCatch
    model <- tryCatch({
      qda(Label ~ ., data = train.data)
    }, error = function(e) {
      return(NULL)  # Return NULL if QDA fails
    })
    if (is.null(model)) return(NA)  # Return NA if QDA fails
    pred <- predict(model, test.data)$class
  } else if (method == "Naive Bayes") {
    model <- naiveBayes(Label ~ ., data = train.data)
    pred <- predict(model, test.data)
  } else if (method == "SVM Poly d=3") {
    model <- svm(Label ~ ., data = train.data, kernel = "polynomial", degree = 3)
    pred <- predict(model, test.data)
  } else if (method == "SVM RBF") {
    model <- svm(Label ~ ., data = train.data, kernel = "radial")
    pred <- predict(model, test.data)
  } else if (method == "SVM Linear") {
    model <- svm(Label ~ ., data = train.data, kernel = "linear")
    pred <- predict(model, test.data)
  } else if (method == "Multinomial Logistic") {
    # Increase the maxNWts limit to avoid "too many weights" error
    options(maxNWts = 5000)

    # Fit multinomial logistic regression with trace set to FALSE to suppress output
    model <- tryCatch({
      multinom(Label ~ ., data = train.data, MaxNWts = 5000, trace = FALSE)
    }, error = function(e) {
      return(NULL)  # Return NULL if multinom fails
    })
    if (is.null(model)) return(NA)  # Return NA if multinom fails
    pred <- predict(model, test.data)
  }
  mean(pred != test.data$Label, na.rm = TRUE)  # Calculate MCE
}



# Loop through dimensions and methods with progress bar
methods <- c("LDA", "QDA", "Naive Bayes", "SVM Poly d=3", "SVM RBF", "SVM Linear", "Multinomial Logistic")

# Total number of iterations
total_iterations <- length(p.grid) * length(methods)

# Initialize the progress bar
pb <- progress_bar$new(
  format = "  Processing [:bar] :percent (:current/:total) ETA: :eta",
  total = total_iterations, clear = FALSE, width = 60
)

for (p in p.grid) {
  V <- foo$vectors[, 1:p]  # Select top p principal components
  train.X.p <- as.data.frame(XC %*% V)  # Project training data
  test.X.p <- as.data.frame(scale(test.X, center = colMeans(train.X), scale = FALSE) %*% V)  # Project test data
  
  # Combine with labels
  train.data <- data.frame(train.X.p, Label = as.factor(train.y))
  test.data <- data.frame(test.X.p, Label = as.factor(test.y))
  
  # Evaluate each method
  for (method in methods) {
    mce <- calculate_mce(method, train.data, test.data)
    if (!is.na(mce)) {  # Only add results if MCE is valid
      results <- rbind(results, data.frame(Dimensions = p, MCE_test = mce, Method = method))
    }
    pb$tick()  # Update the progress bar
  }
}

# Plot the results using ggplot2
ggplot(results, aes(x = Dimensions, y = MCE_test, color = Method, linetype = Method)) +
  geom_line(size = 1) +
  labs(
    title = "Test MCE vs Number of Dimensions",
    x = "Number of Dimensions (p)",
    y = "Test Mean Classification Error (MCE)",
    color = "Method"
  ) +
  theme_minimal() +
  theme(legend.position = "bottom", legend.box = "horizontal")

# Save the plot to a file
ggsave(
  filename = "mce_vs_dimensions.png",   # Filename of the output plot
  width = 8,                            # Width of the saved image in inches
  height = 6, dpi = 300                 # Height of the saved image in inches
)

```
