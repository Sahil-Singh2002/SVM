---
title: "Optimized SVM and Classification Models with Cross-Validation"
author: "Sahil"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
# Setup options
knitr::opts_chunk$set(echo = TRUE)
```

### Required Libraries
```{r}
# Load required libraries
library(ggplot2)
library(MASS)     # For LDA/QDA
library(e1071)    # For SVM and Naive Bayes
library(nnet)     # For multinomial logistic regression
library(reshape2) # For reshaping data
```

### Utility Functions
#### PCA Reduction
```{r}
reduce_dimensions <- function(data, n_components = 2) {
  non_constant_columns <- apply(data, 2, function(col) var(col, na.rm = TRUE) > 0)
  data <- data[, non_constant_columns, drop = FALSE]
  
  if (ncol(data) > n_components) {
    pca <- prcomp(data, center = TRUE, scale. = TRUE)
    reduced <- pca$x[, 1:n_components]
  } else {
    reduced <- data
  }
  return(reduced)
}
```

#### Grid Creation
```{r}
create_grid <- function(train_data, test_data, resolution = 200) {
  x_min <- min(c(train_data$Feature1, test_data$Feature1)) - 1
  x_max <- max(c(train_data$Feature1, test_data$Feature1)) + 1
  y_min <- min(c(train_data$Feature2, test_data$Feature2)) - 1
  y_max <- max(c(train_data$Feature2, test_data$Feature2)) + 1
  
  grid <- expand.grid(
    Feature1 = seq(x_min, x_max, length.out = resolution),
    Feature2 = seq(y_min, y_max, length.out = resolution)
  )
  return(grid)
}
```

#### Cross-Validation
```{r}
perform_cross_validation <- function(model_func, data, k = 5) {
  folds <- sample(rep(1:k, length.out = nrow(data)))
  errors <- numeric(k)
  
  for (i in 1:k) {
    train_data <- data[folds != i, ]
    test_data <- data[folds == i, ]
    
    model <- model_func(train_data)
    test_pred <- predict(model, test_data)
    
    if (is.list(test_pred) && "class" %in% names(test_pred)) {
      test_pred <- test_pred$class
    }
    
    errors[i] <- mean(test_pred != test_data$Label)
  }
  
  return(mean(errors))
}
```

#### Plotting Decision Boundaries
```{r}
plot_decision_boundary <- function(train_data, test_data, grid_points, title) {
  ggplot() +
    geom_tile(data = grid_points, aes(x = Feature1, y = Feature2, fill = Label), alpha = 0.3) +
    geom_point(data = train_data, aes(x = Feature1, y = Feature2, color = Label), alpha = 0.7) +
    geom_point(data = test_data, aes(x = Feature1, y = Feature2, color = Label), shape = 17, alpha = 0.7) +
    geom_contour(
      data = grid_points,
      aes(x = Feature1, y = Feature2, z = as.numeric(Label)),
      color = "black",
      breaks = seq(1.5, length(unique(train.y)), by = 1) # One contour line per class
    ) +
    labs(
      title = title,
      x = "Feature 1",
      y = "Feature 2",
      color = "Labels"
    ) +
    scale_fill_brewer(palette = "Set3") +
    scale_color_brewer(palette = "Set3") +
    theme_minimal()+
    theme(legend.position = "bottom",legend.box = "horizontal")
}

```

### Data Preparation
```{r}
load("C:/Users/pmyss22/OneDrive - The University of Nottingham/Desktop/SML/Group Poject/zipCodeAllDigits.RData")

train.X.reduced <- reduce_dimensions(train.X)
test.X.reduced <- reduce_dimensions(test.X)

train.data <- data.frame(
  Feature1 = train.X.reduced[, 1],
  Feature2 = train.X.reduced[, 2],
  Label = as.factor(train.y)
)

test.data <- data.frame(
  Feature1 = test.X.reduced[, 1],
  Feature2 = test.X.reduced[, 2],
  Label = as.factor(test.y)
)

grid_points <- create_grid(train.data, test.data)
```

#### Model Evaluation
```{r}
train_and_evaluate <- function(model, train_data, test_data, grid_points) {
  # Function to extract class predictions (works for all models)
  get_class_predictions <- function(prediction) {
    if (is.list(prediction) && "class" %in% names(prediction)) {
      return(prediction$class)  # For LDA, QDA
    } else {
      return(prediction)  # For SVM, Naive Bayes, etc.
    }
  }
  
  # Predict on training data
  train.pred <- get_class_predictions(predict(model, train_data))
  train.error <- mean(train.pred != train_data$Label)
  
  # Predict on test data
  test.pred <- get_class_predictions(predict(model, test_data))
  test.error <- mean(test.pred != test_data$Label)
  
  # Predict on grid points
  grid_points$Label <- get_class_predictions(predict(model, grid_points))
  
  return(list(
    train.error = train.error,
    test.error = test.error,
    grid_points = grid_points
  ))
}


```

### Models with Cross-Validation
#### LDA
```{r}
lda_func <- function(data) lda(Label ~ Feature1 + Feature2, data = data)
lda_cv_error <- perform_cross_validation(lda_func, train.data)
lda_model <- lda_func(train.data)
```

#### QDA
```{r}
qda_func <- function(data) qda(Label ~ Feature1 + Feature2, data = data)
qda_cv_error <- perform_cross_validation(qda_func, train.data)
qda_model <- qda_func(train.data)
```

#### Naive Bayes
```{r}
nb_func <- function(data) naiveBayes(Label ~ Feature1 + Feature2, data = data)
nb_cv_error <- perform_cross_validation(nb_func, train.data)
nb_model <- nb_func(train.data)
```

#### SVM (Polynomial Kernel)
```{r}
svm_poly_func <- function(data) svm(Label ~ Feature1 + Feature2, data = data, kernel = "polynomial", degree = 3, cost = 1)
svm_poly_cv_error <- perform_cross_validation(svm_poly_func, train.data)
svm_poly_model <- svm_poly_func(train.data)
```

#### SVM (RBF Kernel)
```{r}
svm_rbf_func <- function(data) svm(Label ~ Feature1 + Feature2, data = data, kernel = "radial")
svm_rbf_cv_error <- perform_cross_validation(svm_rbf_func, train.data)
svm_rbf_model <- svm_rbf_func(train.data)
```

#### SVM (multinomial logistic regresion)
```{r}

multi_log_func <- function(data) multinom(Label ~ Feature1 + Feature2, data = data)
multi_log_cv_error <- perform_cross_validation(multi_log_func, train.data)
multi_log_model <- multi_log_func(train.data)
```

### Plotting Decision Boundaries
You can plot decision boundaries for each model trained on the full training dataset:
```{r}
# Save LDA Decision Boundary
lda_results <- train_and_evaluate(lda_model, train.data, test.data, grid_points)
lda_plot <- plot_decision_boundary(train.data, test.data, lda_results$grid_points, "LDA Decision Boundary")
ggsave("LDA_Decision_Boundary.png", plot = lda_plot, width = 8, height = 6)

# Save QDA Decision Boundary
qda_results <- train_and_evaluate(qda_model, train.data, test.data, grid_points)
qda_plot <- plot_decision_boundary(train.data, test.data, qda_results$grid_points, "QDA Decision Boundary")
ggsave("QDA_Decision_Boundary.png", plot = qda_plot, width = 8, height = 6)

# Save Naive Bayes Decision Boundary
nb_results <- train_and_evaluate(nb_model, train.data, test.data, grid_points)
nb_plot <- plot_decision_boundary(train.data, test.data, nb_results$grid_points, "Naive Bayes Decision Boundary")
ggsave("Naive_Bayes_Decision_Boundary.png", plot = nb_plot, width = 8, height = 6)

# Save SVM Polynomial Decision Boundary
svm_poly_results <- train_and_evaluate(svm_poly_model, train.data, test.data, grid_points)
svm_poly_plot <- plot_decision_boundary(train.data, test.data, svm_poly_results$grid_points, "SVM Polynomial Decision Boundary")
ggsave("SVM_Polynomial_Decision_Boundary.png", plot = svm_poly_plot, width = 8, height = 6)

# Save SVM RBF Decision Boundary
svm_rbf_results <- train_and_evaluate(svm_rbf_model, train.data, test.data, grid_points)
svm_rbf_plot <- plot_decision_boundary(train.data, test.data, svm_rbf_results$grid_points, "SVM RBF Decision Boundary")
ggsave("SVM_RBF_Decision_Boundary.png", plot = svm_rbf_plot, width = 8, height = 6)

# Save SVM Sigmoid (Neural Network) Decision Boundary
multi_log_results <- train_and_evaluate(multi_log_model, train.data, test.data, grid_points)
multi_log_plot <- plot_decision_boundary(train.data, test.data, multi_log_results$grid_points, "Multinomial Logistic regresion Decision Boundary")
ggsave("Multinomial_Logistic_regresion_Decision_Boundary.png", plot = svm_sigmoid_plot, width = 8, height = 6)

```

### Error Comparison
```{r}
# Create the error comparison table
error_comparison <- data.frame(
  Method = c("LDA", "QDA", "Naive Bayes", "SVM Polynomial", "SVM RBF", "Multinomial Logistic Regresion"),
  Training_Error = c(lda_results$train.error, qda_results$train.error, nb_results$train.error, svm_poly_results$train.error, svm_rbf_results$train.error,multi_log_results$train.error),
  Test_Error = c(lda_cv_error, qda_cv_error, nb_cv_error, svm_poly_cv_error, svm_rbf_cv_error,multi_log_cv_error )  # Cross-validation error serves as test error
)

# Print the table
print(error_comparison)

```

```{r}
# Define the error rates for each method
methods <- c("LDA", "QDA", "Naive Bayes", "SVM Polynomial", "SVM RBF", "Multinomial Logistic Regresion")


training_errors <- c(lda_results$train.error, qda_results$train.error, nb_results$train.error, svm_poly_results$train.error, svm_rbf_results$train.error,multi_log_results$train.error)

test_errors <- c(lda_cv_error, qda_cv_error, nb_cv_error, svm_poly_cv_error, svm_rbf_cv_error,multi_log_cv_error )

# Combine into a data frame
comparison_table <- data.frame(
  Method = methods,
  Training_Error = training_errors,
  Test_Error = test_errors
)

# Print the table
print(comparison_table)

# Reshape the data for heatmap plotting
comparison_table_long <- reshape2::melt(comparison_table, id.vars = "Method")

# Generate the heatmap using ggplot2
library(ggplot2)
plotheat_map <- ggplot(comparison_table_long, aes(x = variable, y = Method, fill = value)) +
  geom_tile(color = "white") +
  geom_text(aes(label = sprintf("%.2f", value)), size = 4) +
  scale_fill_gradient(low = "lightblue", high = "darkblue") +
  labs(
    title = "Classification Method Error Comparison",
    x = "Error Type",
    y = "Classification Method",
    fill = "Error"
  ) +
  theme_minimal()

ggsave("Heat Map.png", plot = plotheat_map, width = 8, height = 6)

```

```{r}
# Load necessary libraries
library(plotly)

# Assuming train.X is a matrix and train.y is a numeric vector
# If train.X has more than 3 columns, reduce dimensions using PCA
if (ncol(train.X) > 3) {
  pca <- prcomp(train.X, center = TRUE, scale. = TRUE)
  train.X.reduced <- pca$x[, 1:3]  # Use the first three principal components
} else {
  train.X.reduced <- train.X  # Use the original features if already 3D or fewer
}

# Convert to a data frame
train.data <- data.frame(
  Feature1 = train.X.reduced[, 1],
  Feature2 = train.X.reduced[, 2],
  Feature3 = train.X.reduced[, 3],
  Label = as.factor(train.y)  # Convert labels to factors
)

# Create the 3D scatter plot
fig <- plot_ly(
  train.data,
  x = ~Feature1,
  y = ~Feature2,
  z = ~Feature3,
  color = ~Label,
  colors = "Set3",
  type = "scatter3d",
  mode = "markers",
  marker = list(size = 5, opacity = 0.7)
)

# Add title and axis labels
fig <- fig %>%
  layout(
    title = "3D Scatter Plot of train.X with Labels from train.y",
    scene = list(
      xaxis = list(title = "Feature 1"),
      yaxis = list(title = "Feature 2"),
      zaxis = list(title = "Feature 3")
    )
  )

# Show the plot
fig
```
